1. 神经网络和其他机器学习方法的区别？
也就是说神经网络与其他机器学习方法比较的优势在那里？

1) 特征提取的高效性

大家可能会疑惑，对于同一个分类任务，我们可以用机器学习的算法来做，为什么要用神经网络呢？大家回顾一下，一个分类任务，我们在用机器学习算法来做时，首先要明确feature和label，然后把这个数据"灌"到算法里去训练，最后保存模型，再来预测分类的准确性。但是这就有个问题，即我们需要实现确定好特征，每一个特征即为一个维度，特征数目过少，我们可能无法精确的分类出来，即我们所说的欠拟合，如果特征数目过多，可能会导致我们在分类过程中过于注重某个特征导致分类错误，即过拟合。

　　举个简单的例子，现在有一堆数据集，让我们分类出西瓜和冬瓜，如果只有两个特征：形状和颜色，可能没法分区来；如果特征的维度有：形状、颜色、瓜瓤颜色、瓜皮的花纹等等，可能很容易分类出来；如果我们的特征是：形状、颜色、瓜瓤颜色、瓜皮花纹、瓜蒂、瓜籽的数量，瓜籽的颜色、瓜籽的大小、瓜籽的分布情况、瓜籽的XXX等等，很有可能会过拟合，譬如有的冬瓜的瓜籽数量和西瓜的类似，模型训练后这类特征的权重较高，就很容易分错。这就导致我们在特征工程上需要花很多时间和精力，才能使模型训练得到一个好的效果。然而神经网络的出现使我们不需要做大量的特征工程，譬如提前设计好特征的内容或者说特征的数量等等，我们可以直接把数据灌进去，让它自己训练，自我“修正”，即可得到一个较好的效果。

2)数据格式的简易性

在一个传统的机器学习分类问题中，我们“灌”进去的数据是不能直接灌进去的，需要对数据进行一些处理，譬如量纲的归一化，格式的转化等等，不过在神经网络里我们不需要额外的对数据做过多的处理

3)参数数目的少量性

在面对一个分类问题时，如果用SVM来做，我们需要调整的参数需要调整核函数，惩罚因子，松弛变量等等，不同的参数组合对于模型的效果也不一样，想要迅速而又准确的调到最适合模型的参数需要对背后理论知识的深入了解(当然，如果你说全部都试一遍也是可以的，但是花的时间可能会更多),对于一个基本的三层神经网络来说(输入-隐含-输出)，我们只需要初始化时给每一个神经元上随机的赋予一个权重w和偏置项b，在训练过程中，这两个参数会不断的修正，调整到最优质，使模型的误差最小。所以从这个角度来看，我们对于调参的背后理论知识并不需要过于精通(只不过做多了之后可能会有一些经验，在初始值时赋予的值更科学，收敛的更快罢了)

2. 为什么要用卷积神经网络？

2.1 传统神经网络的劣势

我们知道，图像是由一个个像素点构成，每个像素点有三个通道，分别代表RGB颜色，那么，如果一个图像的尺寸是（28，28，1），即代表这个图像的是一个长宽均为28，channel为1的图像（channel也叫depth,此处1代表灰色图像）。如果使用全连接的网络结构，即，网络中的神经与与相邻层上的每个神经元均连接，那就意味着我们的网络有28 * 28 =784个神经元，hidden层采用了15个神经元，那么简单计算一下，我们需要的参数个数(w和b)就有：784*15*10+15+10=117625个，这个参数太多了，随便进行一次反向传播计算量都是巨大的，从计算资源和调参的角度都不建议用传统的神经网络。(评论中有同学对这个参数计算不太理解，我简单说一下：图片是由像素点组成的，用矩阵表示的，28*28的矩阵，肯定是没法直接放到神经元里的，我们得把它“拍平”，变成一个28*28=784 的一列向量，这一列向量和隐含层的15个神经元连接，就有784*15=11760个权重w，隐含层和最后的输出层的10个神经元连接，就有11760*10=117600个权重w，再加上隐含层的偏置项15个和输出层的偏置项10个，就是：117625个参数了)

2.2 卷积神经网络的构成

https://blog.csdn.net/v_JULY_v/article/details/51812459
https://www.cnblogs.com/charlotte77/p/7759802.html
上面的两个链接给出了非常详细和生动的解释

2.2.1 卷积层(Convolutional layer）


2.2.2 池化层(pooling Layer)

池化层的主要目的是通过降采样的方式，在不影响图像质量的情况下，压缩图片，减少参数。
